---
title: "Credit Risk Analysis with R"
author: "Felipe Solares"
date: "08/01/2020"
output: html_document
---
### **About**
This is a  credit risk analysis using 'R' and Machine Learning developed by Felipe Solares da Silva. This is part of his *professional portfolio* and if you want to see more projects like this, go and check my portfolio at https://github.com/fsolares/professional-portfolio.

**Contact: solares.fs@gmail.com**

### **Acknowledgment**

Thank you **Jaques D'Erasmo** (https://github.com/Jaquesd), old friend and also Data Science student for your immeasurable contribution on this project, all your feedback, code sujection and support during my path gave me the strength to overcome this challenge. Congratualation for us, that was an ammazing and real team work.

### **Versions**

- Windows OS     release  10
- machine        AMD64
- processor      Intel64 Family 6 Model 58 Stepping 9, GenuineIntel
- platform       x86_64-w64-mingw32          
- arch           x86_64                      
- os             mingw32                     
- system         x86_64, mingw32             
- svn rev        76782                       
- language       R                           
- version.string R version 3.6.1 (2019-07-05)

### **Credit Risk Analysis**

#### **Project Purpose**

Perform a credit risk analysis using two different machine learning models: *Boosted Decision Tree* and *Boosted Logistic Regression*. The main goal is to identify the possibility of a loss resulting from a borrower's failure to repay a loan or meet contractual obligations.

### **Step 1 - Importing Essential Packages and Modules**



- 1 - DMwR - Allows us to balance the data set using sampling methods (SMOTE).
- 2 - caret - Provides tools for machine learning activities.
- 3 - scales - Scaling and Centering of Matrix-like Objects.
- 4 - randomForest -  Implements Breiman's random forest algorithm for classification and regression.
- 5 - ggplot2 - Create Elegant Data Visualisations Using the Grammar of Graphics.
- 6 - dplyr - Provides tools for manipulating datasets.
- 7 - ada - Performs discrete, real, and gentle boost for classification.
- 8 - forcats - Reorder Factor Levels By Sorting Along Another Variable.
- 9 - ROCR - Visualizing the Performance of Scoring Classifiers.

If you don't have any of these packages already installed in your rstudio please, run the code below!

```{r echo=TRUE, message=FALSE, warning=FALSE, results = 'hide'}
chooseCRANmirror(graphics=FALSE, ind=1) # No need to run this part, 
                                        # it's a R Markdown correction.

packs <- c('DMwR', 'caret', 'scales', 'randomForest',
           'ggplot2', 'dplyr', 'ada', 'forcats', 'ROCR')
for(p in packs){
  install.packages(p)
}
```

If you have some of this packages, please run the code below giving the name of the package that are missing for you!

```{r echo=TRUE, message=FALSE, warning=FALSE, results = 'hide'}
chooseCRANmirror(graphics=FALSE, ind=1) # No need to run this part, 
                                        # it's a R Markdown correction.

install.packages('your missing package')
```

#### **Loading Essential Packages**
After the packages are installed, make sure to run the code below to import the modules and make the required connections.

```{r echo=TRUE, message=FALSE, warning=FALSE, results = 'hide'}
chooseCRANmirror(graphics=FALSE, ind=1) # No need to run this part, 
                                        # it's a R Markdown correction.
packs <- c('DMwR', 'caret', 'scales', 'randomForest',
           'ggplot2', 'dplyr', 'ada', 'forcats', 'ROCR')
lapply(packs, require, character.only = T)
```


### **Step 2 - The Data Set**

For this project, we're going to use the famous German Credit Data Set, already cleaned and organized (new column names and target variable in the first position).
You can find the original data set at: https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data).

#### **Collecting Data**

```{r echo=TRUE, message=FALSE, warning=FALSE, results = 'hide'}
df <- read.csv('credit_dataset.csv', header = T, sep = ',', stringsAsFactors = F)

```

#### **Feature Engineering**

This dataset classifies people, described by a set of attributes, as good or bad credit risks. What we have in our hands is a classification problem, so we're going to use all attributes information provided by UCI in order to define categorical and numerical variables. We'll also create a new column called *age_categ* considering age values as a categorical feature (age groups).

##### **Creating a new column**
```{r echo=TRUE, message=FALSE, warning=FALSE, results = 'hide'}

# Summarizing age column to identify max an min values.

summary(df$age)

# Adding a new categorical feature to the data set.

df["age_categ"] <- cut(df$age, breaks = c(15, 30, 45, 60, 75))
```

##### **Transforming and Normalizing**
```{r echo=TRUE, message=FALSE, warning=FALSE, results = 'hide'}

# Creating a function to convert qualitative variables to factors.

tofactor <- function(dataf, var){
  for (variable in var){
    dataf[[variable]] <- as.factor(dataf[[variable]])
  }
  return(dataf)
}

# Creating a function to normalize the numerical variables.

normalizing <- function(dataf, var){
  for(variable in var){
    dataf[[variable]] <- scale(dataf[[variable]], center = T, scale = T)
  }
  return(dataf)
}

# Splitting columns in categoricals and numericals according to UCI dictionary.

categ_vars <- c('credit.rating', 'account.balance', 'previous.credit.payment.status',
                'credit.purpose', 'savings', 'employment.duration', 'installment.rate',
                'marital.status', 'guarantor', 'residence.duration', 'current.assets',
                'other.credits', 'apartment.type', 'bank.credits', 'occupation', 
                'dependents', 'telephone', 'foreign.worker', 'age_categ')

numeric_vars <- colnames(df[, - match(categ_vars, names(df))])


# Normalizing.

df <- normalizing(df, numeric_vars)

# Converting to factor.

df <- tofactor(df, categ_vars)

```


### **Step 3 - Handling Imbalanced Data Sets**

Accuracy, while being an important and unavoidable metric, can be misleading and therefore should be used cautiously and alongside other metrics. Imbalanced data could mislead your models making tendentious predictions, let's take *df* as an example here and check the data.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Numerical Approach

table(df$credit.rating)

```

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align = 'center'}

# Grapical Approach

df %>% 
  ggplot(aes(x = credit.rating, fill = credit.rating)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  geom_text(aes(y = ((..count..)/sum(..count..)), 
                label = percent((..count..)/sum(..count..))), 
                stat = "count", vjust = -0.25) +
  theme(plot.title = element_text(face = "bold")) +
  labs(title="Checking Imbalance Data ", subtitle="analysing the dependent variable",
       caption = 'Source: Data collected from German Credit Data Set', 
       y="Frequencies", x="Credit Rating") +
  scale_x_discrete(breaks = c("0", "1"),
                   labels = c("Bad", "Good")) +
  scale_fill_discrete(name = "Rating", labels = c("Bad", "Good")) +
  scale_y_continuous(labels = percent)
```


It is proven that the data set is unbalanced, the numbers and the charts show us the dominance of good credit borrowers over bad ones. We're not here for questioning the reality of this scenario, but it is accurate to imply that if we train our model giving tendentious observations, it will be tendentious as well causing a "naive behaviour".

There is a good number of techniques to rebalance the data. The chosen method was the Synthetic Minority Over-Sampling Technique (SMOTE). This method is used to avoid overfitting when adding exact replicas of minority instances to the main dataset.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Resampling the data

set.seed(123) # Setting seed for reproducibility.
newdf <- SMOTE(credit.rating ~ ., df, perc.over = 100, perc.under = 200)

# Checking results.

table(newdf$credit.rating)

# Taking the columns names for futher analysis.
colnames <- colnames(newdf)
```


### **Step 4 - Feature Selection**

In this step, we have to decide which features it's important and deeply related to our target variable *credit.rating*. As we are dealing mostly with categorical variables, we're going to compare each one with our target using a graphical approach instead of using numerical methods.


#### **Exploring the data**
```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align = 'center'}

grid.names <- c('BAD', 'GOOD')
names(grid.names) <- c(0, 1)

lapply(colnames[2:22], function(x){
  if(is.factor(newdf[,x])) {
    ggplot(newdf, aes_string(x)) +
      geom_bar() + 
      facet_grid(. ~ credit.rating, labeller = labeller(credit.rating = grid.names)) + 
      labs(title="Credit Risk", subtitle= paste("by column", x),
           caption = 'Source: Data collected from German Credit Data Set', 
           y="Frequencies")}})

```


Even using graphical analysis, relating each variable with our target, it's hard to define those that will make a more accurate model. Therefore we decide to use machine learning models to help us out with featuring selection due to an importance score.

#### **Feature Selection Methods**

In this section, we're going to use ML Azure Studio to perform a feature selection and to test different machine learning models using the features from each method. The one that gets the highest score will be chosen as our model.

From each method we will select five features with highest importance scores.

#####*Using Random Forest*

```{r echo=TRUE, message=FALSE, warning=FALSE, results = 'hide'}
set.seed(123)
rmodel <- randomForest(credit.rating ~ ., data = newdf, ntree = 150, nodesize = 15, importance = T)
```

##### *Evaluating Random Forest*
```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align = 'center', fig.width = 10}
# Evaluating the features using random forest importance plot.

varImpPlot(rmodel)
```

According to Random Forest the five best features should be: account.balance, credit.duration.months, credit.amount, previous.credit.payment.status, other.credits.

##### *Using Learning Vector Quantization (LVQ)*

```{r echo=TRUE, message=FALSE, warning=FALSE, results = 'hide'}
set.seed(123)
control.lvq <- trainControl(method = 'repeatedcv', number = 10, repeats = 3)

lvq.model <- train(credit.rating ~ ., data = newdf, method='lvq', preProcess="scale", trControl=control.lvq)
```

##### *Evaluating Learning Vector Quantization (LVQ)*
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Importance Table

varImp(lvq.model)
```

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align = 'center'}
# Importance Charts

ggplot(varImp(lvq.model)) +
  theme(plot.title = element_text(face = "bold")) +
  labs(title="Feature Selection", subtitle=" by Learning Vector Quantization (LVQ)",
       caption = 'Source: Data collected from German Credit Data Set', 
       y="Importance", x="Features") 
```

According to LVQ the five best features should be: account.balance, credit.duration.months, credit.amount, previous.credit.payment.status, other.credits.

##### *Using Recursive Feature Elimination (RFE)*

```{r echo=TRUE, message=FALSE, warning=FALSE, results = 'hide'}
set.seed(123)
control.rfe <- rfeControl(functions=rfFuncs, method="cv", number=10)
results <- rfe(newdf[,2:22], newdf[,1], sizes=c(1:8), rfeControl=control.rfe)

```

##### *Evaluating Recursive Feature Elimination (RFE)*
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Importance Table

varImp(results)
```

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align = 'center'}
# Importance Charts

rfe.df <- data.frame(varImp(results))
rfe.df <- cbind(rfe.df, row.names(rfe.df))
colnames(rfe.df) <- c('Importance','Features')
rfe.df$Features <- factor(rfe.df$Features)

rfe.df %>%
    mutate(Features = fct_reorder(Features, Importance)) %>% 
    ggplot(aes(x = Features , y = Importance)) +
      geom_bar(stat = 'identity') +
      coord_flip() +
      theme(plot.title = element_text(face = "bold")) +
      labs(title="Feature Selection", subtitle=" by Recursive Feature Elimination (RFE)",
           caption = 'Source: Data collected from German Credit Data Set', 
           y="Importance", x="Features") 
```

According to RFE the five best features should be: account.balance, credit.duration.months, credit.amount, previous.credit.payment.status, credit.purpose.

##### *Using Filter Based Feature Selection from ML Azure*
```{r pressure, echo=FALSE, out.width = '80%' , fig.align='center'}
knitr:: include_graphics("Azure_FeatureSelec.png")
```

According to Azure ML the five best features should be: account.balance, previous.credit.payment.status, credit.duration.months, savings, other.credit.

##### *RESULTS FROM AZURE ML STUDIO*

**Features Selected By Azure**

 - Boosted Decision Tree: 0.765
 - SVM: 0.735
 - Neural Network: 0.738 
 - Logistic Regression: 0.750

**Features Selected By Random Forest**

 - Boosted Decision Tree: 0.773
 - SVM: 0.723
 - Neural Network: 0.731
 - Logistic Regression: 0.738

 **Features Selected By LVQ**
 
 - Boosted Decision Tree: 0.769
 - SVM: 0.712
 - Neural Network: 0.727
 - Logistic Regression: 0.746

 **Features Selected By RFE**

 - Boosted Decision Tree: 0.773
 - SVM: 0.723
 - Neural Network: 0.731
 - Logistic Regression: 0.738
  
  

### **Step 5 - Creating Models**

Based on the results above, we're going to implement the best two models using RFE featuring Selection: Boosted decion tree and Logistic Regression.


#### **Partitioning the Data**
```{r echo=TRUE, message=FALSE, warning=FALSE, results = 'hide'}
# Splitting the data into train and test 80 - 20.

set.seed(123)
idx <- createDataPartition(newdf$credit.rating, p = .8, list = F, times = 1)

df.train <- newdf[idx,]
df.test <- newdf[-idx,]

# Formula
formula <- as.formula('credit.rating ~ account.balance + credit.duration.months + 
                      previous.credit.payment.status + credit.purpose + credit.amount')

```

#### **Training and Evaluating**

##### *Additive Logistic Regression*
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Training Process.

set.seed(123)
ada.model <- ada(formula, data = df.train, type='gentle')

# Predictions.

ada.pred <- predict(ada.model, df.test)
ada.pred.t <- as.numeric(as.character(ada.pred))

# Confusion Matrix.

confusionMatrix(table(ada.pred, df.test$credit.rating), positive = '1')
```

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align = 'center'}
# ROCR Curve for Additive Logistic Regression

set.seed(123)
ada.obj <- prediction(ada.pred.t, df.test$credit.rating) 
ada.performance <- performance(ada.obj, 'tpr', 'fpr')

plot(ada.performance, colorize = T, lty = 1, lwd = 3, 
     main = "ROC Curve")
abline(0,1, col = "black")
auc <- performance(ada.obj, "auc")
auc <- unlist(slot(auc, "y.values"))
auc <- round(auc,2)
legend(0.4,0.4, legend = c(paste0("AUC: ",auc)), cex = 0.6, bty = "n", box.col = "white")

```

##### *Boosted Decision Tree*
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Training Process.
  
set.seed(123)
xgb.model <- train(formula, data = df.train, method = 'xgbTree')

# Predictions.

xgb.pred <- predict(xgb.model,df.test)
xgb.pred.t <- as.numeric(as.character(xgb.pred))


# Confusion Matrix.

confusionMatrix(table(xgb.pred, df.test$credit.rating), positive = '1')
```

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.align = 'center'}
#ROCR Curve for Boosted Decision Tree.

set.seed(123)
xgb.obj <- prediction(xgb.pred.t,df.test$credit.rating)
xgb.performance <- performance(xgb.obj, 'tpr', 'fpr')

plot(xgb.performance, colorize = T, lty = 1, lwd = 3, 
     main = "ROC Curve")
abline(0,1, col = "black")
auc <- performance(xgb.obj, "auc")
auc <- unlist(slot(auc, "y.values"))
auc <- round(auc,2)
legend(0.4,0.4, legend = c(paste0("AUC: ",auc)), cex = 0.6, bty = "n", box.col = "white")
```

#### **Comparing the Models**
``````{r echo=TRUE, message=FALSE, warning=FALSE, fig.align = 'center'}
# XGB vs ADA

plot(ada.performance, col = 'blue', lty = 1, lwd = 3, 
     main = "ROC Curve")
plot(xgb.performance, add = T, col = 'red', lty = 1, lwd = 3, 
     main = "ROC Curve")
legend(0.2,0.7, legend = c('ADA.MODEL'), cex = 0.6, bty = "p", box.col = "blue")
legend(0.05,0.6, legend = c('XGB.MODEL'), cex = 0.6, bty = "p", box.col = "red")
```

The boosted decion tree has the best performance of both, so we'll try to optmize and see if we may achive higher accuracy scores.

#### **Optimizing**

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Training Process.

set.seed(123)
control <- trainControl(method = 'repeatedcv', number = 10, repeats = 3)
model <- train(formula, data = df.train, method='xgbTree', trControl = control)

# Predictions.

pred <- predict(model, df.test)

# Confusion Matrix

confusionMatrix(table(pred, df.test$credit.rating), positive = '1')
```

























